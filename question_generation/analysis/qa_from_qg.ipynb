{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'aux_qa' from '/.auto/home6/mroemmele/CoreNLP/python/question_generation/aux_qa.py'>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import os\n",
    "import json\n",
    "import pandas\n",
    "\n",
    "import importlib\n",
    "import aux_qa\n",
    "importlib.reload(aux_qa)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate generated questions with QA model\n",
    "\n",
    "Objective is for QA model to predict correct answers to generated questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"What was the name of John's Calvert Island?\",\n",
       " \"What was the name of John and Elizabeth's life?\",\n",
       " 'When was she added?',\n",
       " 'Who is Nancy Cappelmann?',\n",
       " 'What did he say?',\n",
       " 'How many died?',\n",
       " 'Where are the happened?',\n",
       " 'Where is the match?',\n",
       " 'What did the cause of the deaths of deaths?',\n",
       " 'How many horses dropped dead?']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Specify which set of generated questions to evaluate on'''\n",
    "\n",
    "output_dir = '/home/mroemmele/question_generation/squad_newsqa_gpt2_tok_no_masking_models/baseline/outputs/'\n",
    "gen_questions_filepath = \"squad_newsqa_test_set_detok.txt\"\n",
    "\n",
    "gen_questions = [question.strip() for question in open(os.path.join(output_dir, gen_questions_filepath))]\n",
    "gen_questions[-10:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>input_text</th>\n",
       "      <th>gold_question</th>\n",
       "      <th>gold_answer</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team represented the AFC at Super Bo...</td>\n",
       "      <td>[Denver Broncos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team represented the NFC at Super Bo...</td>\n",
       "      <td>[Carolina Panthers]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Where did Super Bowl 50 take place?</td>\n",
       "      <td>[Santa Clara, California, Levi's Stadium, Levi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>Which NFL team won Super Bowl 50?</td>\n",
       "      <td>[Denver Broncos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>What color was used to emphasize the 50th anni...</td>\n",
       "      <td>[gold]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>What was the theme of Super Bowl 50?</td>\n",
       "      <td>[\"golden anniversary\", \"golden anniversary, go...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>What day was the game played on?</td>\n",
       "      <td>[February 7, February 7, 2016]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>What is the AFC short for?</td>\n",
       "      <td>[American Football Conference]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>What was the theme of Super Bowl 50?</td>\n",
       "      <td>[\"golden anniversary\", gold, gold-themed]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Super Bowl 50 was an American football game to...</td>\n",
       "      <td>What does AFC stand for?</td>\n",
       "      <td>[American Football Conference]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          input_text  \\\n",
       "0  Super Bowl 50 was an American football game to...   \n",
       "1  Super Bowl 50 was an American football game to...   \n",
       "2  Super Bowl 50 was an American football game to...   \n",
       "3  Super Bowl 50 was an American football game to...   \n",
       "4  Super Bowl 50 was an American football game to...   \n",
       "5  Super Bowl 50 was an American football game to...   \n",
       "6  Super Bowl 50 was an American football game to...   \n",
       "7  Super Bowl 50 was an American football game to...   \n",
       "8  Super Bowl 50 was an American football game to...   \n",
       "9  Super Bowl 50 was an American football game to...   \n",
       "\n",
       "                                       gold_question  \\\n",
       "0  Which NFL team represented the AFC at Super Bo...   \n",
       "1  Which NFL team represented the NFC at Super Bo...   \n",
       "2                Where did Super Bowl 50 take place?   \n",
       "3                  Which NFL team won Super Bowl 50?   \n",
       "4  What color was used to emphasize the 50th anni...   \n",
       "5               What was the theme of Super Bowl 50?   \n",
       "6                   What day was the game played on?   \n",
       "7                         What is the AFC short for?   \n",
       "8               What was the theme of Super Bowl 50?   \n",
       "9                           What does AFC stand for?   \n",
       "\n",
       "                                         gold_answer  \n",
       "0                                   [Denver Broncos]  \n",
       "1                                [Carolina Panthers]  \n",
       "2  [Santa Clara, California, Levi's Stadium, Levi...  \n",
       "3                                   [Denver Broncos]  \n",
       "4                                             [gold]  \n",
       "5  [\"golden anniversary\", \"golden anniversary, go...  \n",
       "6                     [February 7, February 7, 2016]  \n",
       "7                     [American Football Conference]  \n",
       "8          [\"golden anniversary\", gold, gold-themed]  \n",
       "9                     [American Football Conference]  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Load test data'''\n",
    "\n",
    "input_texts_filepath = \"/home/mroemmele/question_generation/squad_newsqa_untok_data/test/paragraphs.txt\"\n",
    "gold_questions_filepath = \"/home/mroemmele/question_generation/squad_newsqa_untok_data/test/questions.txt\"\n",
    "gold_answers_filepath = \"/home/mroemmele/question_generation/squad_newsqa_untok_data/test/answers_only.txt\"\n",
    "\n",
    "# For QA, mask answer annotations in inputs\n",
    "answer_annotated_input_texts = [text.strip() for text in open(input_texts_filepath)]\n",
    "input_texts = [re.sub(\"\\s?</?ANSWER>\", \"\", text.strip()) for text in answer_annotated_input_texts]\n",
    "gold_questions = [question.strip() for question in open(gold_questions_filepath)]\n",
    "#Test set consists of duplicate questions which could have more than one correct answer.\n",
    "gold_answers = [answer.strip().split(\"\\t\") for answer in open(gold_answers_filepath)]\n",
    "#Map each question to answer set so all answers can be taken into account during evaluation.\n",
    "assert len(input_texts) == len(gold_questions) == len(gold_answers)\n",
    "pandas.DataFrame({'input_text': input_texts, 'gold_question': gold_questions, 'gold_answer': gold_answers})[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> /.auto/home6/mroemmele/CoreNLP/python/question_generation/aux_qa.py(29)answer_questions()\n",
      "-> answers = br.batch_inference(questions, texts, model, tokenizer, device, MAX_SEQ_LENGTH, batch_size)\n",
      "(Pdb) n\n",
      "Done batch:  0\n",
      "> /.auto/home6/mroemmele/CoreNLP/python/question_generation/aux_qa.py(30)answer_questions()\n",
      "-> answers = [answer[0] if answer[0] != 'Answer not found' else \"\"\n",
      "(Pdb) answers\n",
      "['Denver Broncos', 'Carolina Panthers', \"Levi's Stadium\", 'Denver Broncos', '2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers 24â€“10 to earn their third Super Bowl title. The game was played on February 7, 2016,', 'Super Bowl 50', 'February 7, 2016,', 'Carolina Panthers', 'Super Bowl 50', 'Carolina Panthers']\n",
      "(Pdb) q\n"
     ]
    },
    {
     "ename": "BdbQuit",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBdbQuit\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-6258309403e1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m     batch_pred_answers = [answer.strip() for answer in \n\u001b[1;32m      7\u001b[0m                           aux_qa.answer_questions(input_texts[batch_idx:batch_idx + batch_size], \n\u001b[0;32m----> 8\u001b[0;31m                                                   gen_questions[batch_idx:batch_idx + batch_size])]\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mpred_answers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_pred_answers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.auto/home6/mroemmele/CoreNLP/python/question_generation/aux_qa.py\u001b[0m in \u001b[0;36manswer_questions\u001b[0;34m(texts, questions)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     answers = [answer[0] if answer[0] != 'Answer not found' else \"\"\n\u001b[0m\u001b[1;32m     31\u001b[0m                for answer in answers]\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/.auto/home6/mroemmele/CoreNLP/python/question_generation/aux_qa.py\u001b[0m in \u001b[0;36manswer_questions\u001b[0;34m(texts, questions)\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0manswers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_inference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mquestions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtexts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAX_SEQ_LENGTH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     answers = [answer[0] if answer[0] != 'Answer not found' else \"\"\n\u001b[0m\u001b[1;32m     31\u001b[0m                for answer in answers]\n\u001b[1;32m     32\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0manswers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/miniconda3/envs/python37/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mtrace_dispatch\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m     86\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;31m# None\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'line'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'call'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0marg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/local/miniconda3/envs/python37/lib/python3.7/bdb.py\u001b[0m in \u001b[0;36mdispatch_line\u001b[0;34m(self, frame)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbreak_here\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquitting\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mraise\u001b[0m \u001b[0mBdbQuit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrace_dispatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBdbQuit\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''Run QA on generated questions'''\n",
    "\n",
    "batch_size = 10\n",
    "pred_answers = []\n",
    "for batch_idx in range(0, len(input_texts), batch_size):\n",
    "    batch_pred_answers = [answer.strip() for answer in \n",
    "                          aux_qa.answer_questions(input_texts[batch_idx:batch_idx + batch_size], \n",
    "                                                  gen_questions[batch_idx:batch_idx + batch_size])]\n",
    "    pred_answers.extend(batch_pred_answers)\n",
    "    print(batch_idx)\n",
    "    #if batch_idx and batch_idx % 1024 == 0:\n",
    "    #     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--Return--\n",
      "> <ipython-input-16-b43342cf34e8>(4)<module>()->None\n",
      "-> import pdb;pdb.set_trace()\n",
      "(Pdb) b 5\n",
      "Breakpoint 5 at <ipython-input-16-b43342cf34e8>:5\n",
      "(Pdb) c\n",
      "> <ipython-input-16-b43342cf34e8>(5)<module>()->None\n",
      "-> eval_scores = aux_qa.evaluate(pred_answers, gold_answers[:10])\n",
      "(Pdb) s\n",
      "--Call--\n",
      "> /.auto/home6/mroemmele/CoreNLP/sandbox/question_generation/aux_qa.py(59)evaluate()\n",
      "-> def evaluate(pred_answers, all_correct_answers):\n",
      "(Pdb) r\n",
      "--Return--\n",
      "> /.auto/home6/mroemmele/CoreNLP/sandbox/question_generation/aux_qa.py(82)evaluate()->{'f1': 0.7, 'precision': 0.7, 'recall': 0.7}\n",
      "-> return {'precision': mean_precision, 'recall': mean_recall, 'f1': mean_f1}\n",
      "(Pdb) precision_scores\n",
      "[1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0]\n",
      "(Pdb) pred_answers[3]\n",
      "'Denver Broncos'\n",
      "(Pdb) all_correct_answers[3]\n",
      "['Carolina Panthers']\n",
      "(Pdb) pred_answers[4]\n",
      "'Denver Broncos'\n",
      "(Pdb) pred_answers[5]\n",
      "'Denver Broncos'\n",
      "(Pdb) all_correct_answers[5]\n",
      "['Carolina Panthers']\n",
      "(Pdb) c\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'precision': 0.7, 'recall': 0.7, 'f1': 0.7}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Evaluate answers'''\n",
    "\n",
    "n_answers = None\n",
    "assert len(pred_answers[:n_answers]) == len(gold_answers[:n_answers])\n",
    "# import pdb;pdb.set_trace()\n",
    "eval_scores = aux_qa.evaluate(pred_answers[:n_answers], gold_answers[:n_answers])\n",
    "eval_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Print sample of output'''\n",
    "\n",
    "for (input_text, gen_question, pred_answer, \n",
    "     gold_question, gold_answer) in zip(answer_annotated_input_texts[:1000], gen_questions[:1000], pred_answers[:1000], \n",
    "                                        gold_questions[:1000], gold_answers[:1000]):\n",
    "    print(\"TEXT:\", input_text)\n",
    "    print(\"GEN QUESTION:\", gen_question)\n",
    "    print(\"PRED ANSWER:\", pred_answer)\n",
    "    print(\"HUMAN QUESTION:\", gold_question)\n",
    "    print(\"CORRECT ANSWER:\", gold_answer)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Create directory for QA evaluation output'''\n",
    "\n",
    "if not os.path.isdir(os.path.join(output_dir, 'qa_eval')):\n",
    "    os.mkdir(os.path.join(output_dir, 'qa_eval'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save full visualization of output'''\n",
    "\n",
    "with open(os.path.join(output_dir, \"qa_eval/squad_test_full_qa_output_paragraphs.txt\"), 'w') as f:\n",
    "    for (input_text, gen_question, pred_answer, \n",
    "         gold_question, gold_answer) in zip(answer_annotated_input_texts, gen_questions, pred_answers, \n",
    "                                            gold_questions, gold_answers):\n",
    "        \n",
    "        f.write(\"TEXT: {}\\n\".format(input_text))\n",
    "        f.write(\"GEN QUESTION: {}\\n\".format(gen_question))\n",
    "        f.write(\"PRED ANSWER: {}\\n\".format(pred_answer))\n",
    "        f.write(\"HUMAN QUESTION: {}\\n\".format(gold_question))\n",
    "        f.write(\"CORRECT ANSWER: {}\\n\".format(gold_answer))\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save predicted answers'''\n",
    "\n",
    "with open(os.path.join(output_dir, \"qa_eval/squad_test_pred_answers_paragraphs.txt\"), 'w') as f:\n",
    "    f.write(\"\\n\".join(pred_answers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Save evaluation scores'''\n",
    "\n",
    "with open(os.path.join(output_dir, \"qa_eval/squad_test_eval_scores_paragraphs.json\"), 'w') as f:\n",
    "    json.dump(eval_scores, f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
